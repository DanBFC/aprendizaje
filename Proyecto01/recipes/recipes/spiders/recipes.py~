# import scrapy
# from scrapy.linkextractors import LinkExtractor
# from scrapy.contrib.spiders import CrawlSpider, Rule
# from scrapy.contrib.linkextractors import lxmlLinkExtractor


# class QuotesSpider(scrapy.Spider):
#     name = "recipes"
#     allowed_domains = ["cookingchanneltv.com"]
#     urls = ['https://www.cookingchanneltv.com/recipes/articles/100-traditional-italian-recipes']    

#     rules = (Rule(LxmlLinkExtractor(allow()), callback = 'parse', follos = True),)
    
    # def start_requests(self):
    #     urls = ['https://www.cookingchanneltv.com/recipes/articles/100-traditional-italian-recipes']
    #     for url in urls:
    #         yield scrapy.Request(url=url, callback=self.parse)

    

            
            
    # def parse(self, response):
    #     #page = response.url.split("/")[-2]
    #     for link in LxmlLinkExtractor(allow = (), deny = self.allowed_domains).extract_links(response):
    #         item = someItem()
    #         item['url'] = link.url
    #     filename = 'recipes-%s.html' % page
    #     with open(filename, 'wb') as f:
    #         f.write(response.body)
    #     self.log('Saved file %s' % filename)

